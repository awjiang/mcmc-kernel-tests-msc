
@article{grazian_review_2020,
	title = {A review of approximate {Bayesian} computation methods via density estimation: {Inference} for simulator‐models},
	volume = {12},
	issn = {1939-5108, 1939-0068},
	shorttitle = {A review of approximate {Bayesian} computation methods via density estimation},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1486},
	doi = {10.1002/wics.1486},
	language = {en},
	number = {4},
	urldate = {2020-09-09},
	journal = {WIREs Computational Statistics},
	author = {Grazian, Clara and Fan, Yanan},
	month = jul,
	year = {2020}
}

@article{bardenet_markov_2015,
	title = {On {Markov} chain {Monte} {Carlo} methods for tall data},
	url = {http://arxiv.org/abs/1505.02827},
	abstract = {Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number \$n\$ of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis-Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach which samples from a distribution provably close to the posterior distribution of interest, yet can require less than \$O(n)\$ data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor.},
	urldate = {2020-09-08},
	journal = {arXiv:1505.02827 [stat]},
	author = {Bardenet, Rémi and Doucet, Arnaud and Holmes, Chris},
	month = may,
	year = {2015},
	note = {arXiv: 1505.02827},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology}
}

@article{korattikara_austerity_2014,
	title = {Austerity in {MCMC} {Land}: {Cutting} the {Metropolis}-{Hastings} {Budget}},
	shorttitle = {Austerity in {MCMC} {Land}},
	url = {http://arxiv.org/abs/1304.5299},
	abstract = {Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.},
	urldate = {2020-09-08},
	journal = {arXiv:1304.5299 [cs, stat]},
	author = {Korattikara, Anoop and Chen, Yutian and Welling, Max},
	month = feb,
	year = {2014},
	note = {arXiv: 1304.5299},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{lloyd_statistical_2015,
	title = {Statistical {Model} {Criticism} using {Kernel} {Two} {Sample} {Tests}},
	abstract = {We propose an exploratory approach to statistical model criticism using maximum mean discrepancy (MMD) two sample tests. Typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model. MMD two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy. We demonstrate on synthetic data that the selected statistic, called the witness function, can be used to identify where a statistical model most misrepresents the data it was trained on. We then apply the procedure to real data where the models being assessed are restricted Boltzmann machines, deep belief networks and Gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on.},
	language = {en},
	author = {Lloyd, James R and Ghahramani, Zoubin},
	year = {2015},
	pages = {9}
}

@article{liu_kernelized_2016,
	title = {A {Kernelized} {Stein} {Discrepancy} for {Goodness}-of-fit {Tests}},
	abstract = {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein’s identity with the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model ﬁts a set of observations, and derive a new class of powerful goodness-of-ﬁt tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.},
	language = {en},
	author = {Liu, Qiang and Lee, Jason D and Jordan, Michael},
	year = {2016},
	pages = {9}
}

@inproceedings{bonferroni_il_1935,
	address = {Rome, Italy},
	title = {Il calcolo delle assicurazioni su gruppi di teste},
	author = {Bonferroni, C.},
	year = {1935},
	pages = {13--60}
}

@article{steinwart_uence_2001,
	title = {On the {Inﬂuence} of the {Kernel} on the {Consistency} of {Support} {Vector} {Machines}},
	abstract = {In this article we study the generalization abilities of several classiﬁers of support vector machine (SVM) type using a certain class of kernels that we call universal. It is shown that the soft margin algorithms with universal kernels are consistent for a large class of classiﬁcation problems including some kind of noisy tasks provided that the regularization parameter is chosen well. In particular we derive a simple suﬃcient condition for this parameter in the case of Gaussian RBF kernels. On the one hand our considerations are based on an investigation of an approximation property—the so-called universality—of the used kernels that ensures that all continuous functions can be approximated by certain kernel expressions. This approximation property also gives a new insight into the role of kernels in these and other algorithms. On the other hand the results are achieved by a precise study of the underlying optimization problems of the classiﬁers. Furthermore, we show consistency for the maximal margin classiﬁer as well as for the soft margin SVM’s in the presence of large margins. In this case it turns out that also constant regularization parameters ensure consistency for the soft margin SVM’s. Finally we prove that even for simple, noise free classiﬁcation problems SVM’s with polynomial kernels can behave arbitrarily badly.},
	language = {en},
	author = {Steinwart, Ingo},
	year = {2001},
	pages = {27}
}

@article{sriperumbudur_hilbert_2010,
	title = {Hilbert {Space} {Embeddings} and {Metrics} on {Probability} {Measures}},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be deﬁned as the distance between distribution embeddings: we denote this as γk, indexed by the kernel function k that deﬁnes the inner product in the RKHS.},
	language = {en},
	author = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Scholkopf, Bernhard and Lanckriet, Gert R G},
	year = {2010},
	pages = {45}
}

@book{serfling_approximation_2002,
	address = {New York, NY},
	edition = {Paperback ed},
	series = {Wiley series in probability and statistics},
	title = {Approximation theorems of mathematical statistics},
	isbn = {978-0-471-21927-9},
	language = {eng},
	publisher = {Wiley},
	author = {Serfling, Robert J.},
	year = {2002},
	note = {OCLC: 49527610}
}

@article{reddi_decreasing_2014,
	title = {On the {Decreasing} {Power} of {Kernel} and {Distance} based {Nonparametric} {Hypothesis} {Tests} in {High} {Dimensions}},
	url = {http://arxiv.org/abs/1406.2083},
	abstract = {This paper is about two related decision theoretic problems, nonparametric two-sample testing and independence testing. There is a belief that two recently proposed solutions, based on kernels and distances between pairs of points, behave well in high-dimensional settings. We identify different sources of misconception that give rise to the above belief. Specifically, we differentiate the hardness of estimation of test statistics from the hardness of testing whether these statistics are zero or not, and explicitly discuss a notion of "fair" alternative hypotheses for these problems as dimension increases. We then demonstrate that the power of these tests actually drops polynomially with increasing dimension against fair alternatives. We end with some theoretical insights and shed light on the {\textbackslash}textit\{median heuristic\} for kernel bandwidth selection. Our work advances the current understanding of the power of modern nonparametric hypothesis tests in high dimensions.},
	urldate = {2020-09-07},
	journal = {arXiv:1406.2083 [cs, math, stat]},
	author = {Reddi, Sashank J. and Ramdas, Aaditya and Póczos, Barnabás and Singh, Aarti and Wasserman, Larry},
	month = nov,
	year = {2014},
	note = {arXiv: 1406.2083},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology}
}

@article{karlsson_corrigendum_2017,
	title = {Corrigendum to “{Bayesian} reduced rank regression in econometrics” [{J}. {Econometrics} 75 (1996) 121–146]},
	volume = {201},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407615002754},
	doi = {10.1016/j.jeconom.2012.10.005},
	language = {en},
	number = {1},
	urldate = {2020-09-05},
	journal = {Journal of Econometrics},
	author = {Karlsson, Sune},
	month = nov,
	year = {2017},
	pages = {170--171}
}

@book{brooks_handbook_2011,
	edition = {0},
	title = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	isbn = {978-1-4200-7942-5},
	url = {https://www.taylorfrancis.com/books/9781420079425},
	language = {en},
	urldate = {2020-09-05},
	publisher = {Chapman and Hall/CRC},
	editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	month = may,
	year = {2011},
	doi = {10.1201/b10905}
}

@article{dedecker_new_2005,
	title = {New dependence coefficients. {Examples} and applications to statistics},
	volume = {132},
	doi = {10.1007/s00440-004-0394-3},
	journal = {Probability Theory and Related Fields},
	author = {Dedecker, Jérôme and Prieur, Clémentine},
	year = {2005},
	pages = {203--236}
}

@article{shao_dependent_2010,
	title = {The {Dependent} {Wild} {Bootstrap}},
	volume = {105},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1198/jasa.2009.tm08744},
	doi = {10.1198/jasa.2009.tm08744},
	language = {en},
	number = {489},
	urldate = {2020-09-05},
	journal = {Journal of the American Statistical Association},
	author = {Shao, Xiaofeng},
	month = mar,
	year = {2010},
	pages = {218--235}
}

@article{jones_markov_2004,
	title = {On the {Markov} chain central limit theorem},
	volume = {1},
	issn = {1549-5787},
	url = {http://projecteuclid.org/euclid.ps/1104335301},
	doi = {10.1214/154957804100000051},
	language = {en},
	number = {0},
	urldate = {2020-09-04},
	journal = {Probability Surveys},
	author = {Jones, Galin L.},
	year = {2004},
	pages = {299--320}
}

@article{gelman_correction_2017,
	title = {Correction to {Cook}, {Gelman}, and {Rubin} (2006)},
	volume = {26},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1377082},
	doi = {10.1080/10618600.2017.1377082},
	language = {en},
	number = {4},
	urldate = {2020-09-04},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gelman, Andrew},
	month = oct,
	year = {2017},
	pages = {940--940}
}

@article{cook_validation_2006,
	title = {Validation of {Software} for {Bayesian} {Models} {Using} {Posterior} {Quantiles}},
	volume = {15},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/106186006X136976},
	doi = {10.1198/106186006X136976},
	language = {en},
	number = {3},
	urldate = {2020-09-04},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cook, Samantha R and Gelman, Andrew and Rubin, Donald B},
	month = sep,
	year = {2006},
	pages = {675--692}
}

@article{talts_validating_2018,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	url = {http://arxiv.org/abs/1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2020-09-04},
	journal = {arXiv:1804.06788 [stat]},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.06788},
	keywords = {Statistics - Methodology}
}

@inproceedings{christmann_universal_2010,
	title = {Universal kernels on non-standard input spaces},
	booktitle = {in {Advances} in {Neural} {Information} {Processing} {Systems}},
	author = {Christmann, Andreas and Steinwart, Ingo},
	year = {2010},
	pages = {406--414}
}

@inproceedings{gartner_graph_2003,
	title = {On {Graph} {Kernels}: {Hardness} {Results} and {Efficient} {Alternatives}},
	booktitle = {{COLT}},
	author = {Gärtner, T. and Flach, Peter A. and Wrobel, S.},
	year = {2003}
}

@inproceedings{vishwanathan_fast_2006,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'06},
	title = {Fast {Computation} of {Graph} {Kernels}},
	abstract = {Using extensions of linear algebra concepts to Reproducing Kernel Hilbert Spaces (RKHS), we define a unifying framework for random walk kernels on graphs. Reduction to a Sylvester equation allows us to compute many of these kernels in O(n3) worst-case time. This includes kernels whose previous worst-case time complexity was O(n6), such as the geometric kernels of Gärtner et al. [1] and the marginal graph kernels of Kashima et al. [2]. Our algebra in RKHS allow us to exploit sparsity in directed and undirected graphs more effectively than previous methods, yielding sub-cubic computational complexity when combined with conjugate gradient solvers or fixed-point iterations. Experiments on graphs from bioinformatics and other application domains show that our algorithms are often more than 1000 times faster than existing approaches.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Vishwanathan, S. V. N. and Borgwardt, Karsten M. and Schraudolph, Nicol N.},
	year = {2006},
	note = {event-place: Canada},
	pages = {1449--1456}
}

@article{siglidis_grakel_2020,
	title = {{GraKeL}: {A} {Graph} {Kernel} {Library} in {Python}},
	volume = {21},
	number = {54},
	journal = {Journal of Machine Learning Research},
	author = {Siglidis, Giannis and Nikolentzos, Giannis and Limnios, Stratis and Giatsidis, Christos and Skianis, Konstantinos and Vazirgiannis, Michalis},
	year = {2020},
	pages = {1--5}
}

@book{steinwart_support_2008,
	address = {New York, NY},
	series = {Information {Science} and {Statistics}},
	title = {Support {Vector} {Machines}},
	isbn = {978-0-387-77241-7 978-0-387-77242-4},
	url = {http://link.springer.com/10.1007/978-0-387-77242-4},
	language = {en},
	urldate = {2020-08-28},
	publisher = {Springer New York},
	author = {Steinwart, Ingo and Christmann, Andreas},
	year = {2008},
	doi = {10.1007/978-0-387-77242-4},
	note = {ISSN: 1613-9011}
}

@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {00359246},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {http://doi.wiley.com/10.1111/j.2517-6161.1995.tb02031.x},
	doi = {10.1111/j.2517-6161.1995.tb02031.x},
	language = {en},
	number = {1},
	urldate = {2020-08-26},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	month = jan,
	year = {1995},
	pages = {289--300}
}

@article{konev_luminescence_1975,
	title = {[{Luminescence} study of the effect of temperature on the conformational state of fibrinogen]},
	volume = {20},
	issn = {0006-3029},
	abstract = {Results are presented of measuring fibrinogen fluorescence parameters in temperature range of 20-80 degrees C at different pH of the solution. It was found that the temperature increase from 20 to 40 degrees C for solutions with pH of 4,5-9,3 were not accompanied by the conformational changes of fibrinogen macromolecules. In the temperature range of 40-50 degrees C for neutral solutions conformational reconstruction of fibrinogen of undenatured character took place. Temperature increase above 50-55 degrees C brings about significant structural changes of fibrinogen molecule which are of denaturation nature.},
	language = {rus},
	number = {4},
	journal = {Biofizika},
	author = {Konev, S. V. and Katibnikov, M. A. and Bandarin, V. A. and Niamaa, D. and Barkovskiĭ, E. V.},
	month = aug,
	year = {1975},
	pmid = {95},
	keywords = {Fibrinogen, Hydrogen-Ion Concentration, Molecular Conformation, Spectrometry, Fluorescence, Temperature},
	pages = {586--590}
}

@book{lehmann_testing_2005,
	address = {New York},
	edition = {3rd ed},
	series = {Springer texts in statistics},
	title = {Testing statistical hypotheses},
	isbn = {978-0-387-98864-1},
	publisher = {Springer},
	author = {Lehmann, E. L. and Romano, Joseph P.},
	year = {2005},
	keywords = {Statistical hypothesis testing}
}

@article{chwialkowski_wild_2016,
	title = {A {Wild} {Bootstrap} for {Degenerate} {Kernel} {Tests}},
	url = {http://arxiv.org/abs/1408.5404},
	abstract = {A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and nondegenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. The code is available at https://github.com/kacperChwialkowski/ wildBootstrap.},
	language = {en},
	urldate = {2020-08-25},
	journal = {arXiv:1408.5404 [stat]},
	author = {Chwialkowski, Kacper and Sejdinovic, Dino and Gretton, Arthur},
	month = sep,
	year = {2016},
	note = {arXiv: 1408.5404},
	keywords = {62G10, Statistics - Machine Learning}
}

@inproceedings{fukumizu_kernel_2007,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'07},
	title = {Kernel {Measures} of {Conditional} {Dependence}},
	isbn = {978-1-60560-352-0},
	abstract = {We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of infinite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Schölkopf, Bernhard},
	year = {2007},
	note = {event-place: Vancouver, British Columbia, Canada},
	pages = {489--496}
}

@book{casella_statistical_1990,
	address = {Pacific Grove, Calif},
	series = {The {Wadsworth} \& {Brooks}/{Cole} {Statistics}/{Probability} series},
	title = {Statistical inference},
	isbn = {978-0-534-11958-4},
	publisher = {Brooks/Cole Pub. Co},
	author = {Casella, George and Berger, Roger L.},
	year = {1990},
	keywords = {Mathematical statistics, Probabilities}
}

@techreport{geweke_evaluating_1991,
	type = {Staff {Report}},
	title = {Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments},
	url = {https://EconPapers.repec.org/RePEc:fip:fedmsr:148},
	abstract = {Data augmentation and Gibbs sampling are two closely related, sampling-based approaches to the calculation of posterior moments. The fact that each produces a sample whose constituents are neither independent nor identically distributed complicates the assessment of convergence and numerical accuracy of the approximations to the expected value of functions of interest under the posterior. In this paper methods for spectral analysis are used to evaluate numerical accuracy formally and construct diagnostics for convergence. These methods are illustrated in the normal linear model with informative priors, and in the Tobit-censored regression model.},
	number = {148},
	institution = {Federal Reserve Bank of Minneapolis},
	author = {Geweke, John},
	year = {1991},
	keywords = {Sampling (Statistics)}
}

@article{gelman_inference_1992,
	title = {Inference from {Iterative} {Simulation} {Using} {Multiple} {Sequences}},
	volume = {7},
	issn = {0883-4237},
	url = {http://projecteuclid.org/euclid.ss/1177011136},
	doi = {10.1214/ss/1177011136},
	language = {en},
	number = {4},
	urldate = {2020-08-24},
	journal = {Statistical Science},
	author = {Gelman, Andrew and Rubin, Donald B.},
	month = nov,
	year = {1992},
	pages = {457--472}
}

@article{del_negro_time_2015,
	title = {Time {Varying} {Structural} {Vector} {Autoregressions} and {Monetary} {Policy}: {A} {Corrigendum}},
	volume = {82},
	issn = {0034-6527, 1467-937X},
	shorttitle = {Time {Varying} {Structural} {Vector} {Autoregressions} and {Monetary} {Policy}},
	url = {https://academic.oup.com/restud/article-lookup/doi/10.1093/restud/rdv024},
	doi = {10.1093/restud/rdv024},
	language = {en},
	number = {4},
	urldate = {2020-08-22},
	journal = {The Review of Economic Studies},
	author = {Del Negro, Marco and Primiceri, Giorgio E.},
	month = oct,
	year = {2015},
	pages = {1342--1345}
}

@article{green_reversible_1995,
	title = {Reversible jump {Markov} chain {Monte} {Carlo} computation and {Bayesian} model determination},
	volume = {82},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/82.4.711},
	doi = {10.1093/biomet/82.4.711},
	language = {en},
	number = {4},
	urldate = {2020-08-22},
	journal = {Biometrika},
	author = {Green, Peter J.},
	year = {1995},
	pages = {711--732}
}

@article{metropolis_equation_1953,
	title = {Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
	volume = {21},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.1699114},
	doi = {10.1063/1.1699114},
	language = {en},
	number = {6},
	urldate = {2020-08-22},
	journal = {The Journal of Chemical Physics},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
	month = jun,
	year = {1953},
	pages = {1087--1092}
}

@article{hastings_monte_1970,
	title = {Monte {Carlo} sampling methods using {Markov} chains and their applications},
	volume = {57},
	issn = {1464-3510, 0006-3444},
	url = {https://academic.oup.com/biomet/article/57/1/97/284580},
	doi = {10.1093/biomet/57.1.97},
	language = {en},
	number = {1},
	urldate = {2020-08-22},
	journal = {Biometrika},
	author = {Hastings, W. K.},
	month = apr,
	year = {1970},
	pages = {97--109}
}

@article{kriege_survey_2020,
	title = {A {Survey} on {Graph} {Kernels}},
	volume = {5},
	issn = {2364-8228},
	url = {http://arxiv.org/abs/1903.11835},
	doi = {10.1007/s41109-019-0195-3},
	abstract = {Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.},
	number = {1},
	urldate = {2020-08-21},
	journal = {Applied Network Science},
	author = {Kriege, Nils M. and Johansson, Fredrik D. and Morris, Christopher},
	month = dec,
	year = {2020},
	note = {arXiv: 1903.11835},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {6}
}

@article{geyer_practical_1992,
	title = {Practical {Markov} {Chain} {Monte} {Carlo}},
	volume = {7},
	issn = {0883-4237},
	url = {http://projecteuclid.org/euclid.ss/1177011137},
	doi = {10.1214/ss/1177011137},
	language = {en},
	number = {4},
	urldate = {2020-08-21},
	journal = {Statistical Science},
	author = {Geyer, Charles J.},
	month = nov,
	year = {1992},
	pages = {473--483}
}

@book{priestley_spectral_1981,
	address = {London ; New York},
	series = {Probability and mathematical statistics},
	title = {Spectral analysis and time series},
	isbn = {978-0-12-564901-8 978-0-12-564902-5},
	publisher = {Academic Press},
	author = {Priestley, M. B.},
	year = {1981},
	keywords = {Spectral theory (Mathematics), Time-series analysis}
}

@book{dedecker_weak_2007,
	address = {New York},
	series = {Lecture notes in statistics},
	title = {Weak dependence: with examples and applications},
	isbn = {978-0-387-69951-6 978-0-387-69952-3},
	shorttitle = {Weak dependence},
	language = {en},
	number = {v.190},
	publisher = {Springer},
	editor = {Dedecker, Jérôme},
	year = {2007},
	keywords = {Dependence (Statistics), Random variables, Stochastic processes}
}

@article{grosse_testing_2014,
	title = {Testing {MCMC} code},
	url = {http://arxiv.org/abs/1412.5218},
	abstract = {Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilistic modeling and inference, but are difficult to debug, and are prone to silent failure if implemented naively. We outline several strategies for testing the correctness of MCMC algorithms. Specifically, we advocate writing code in a modular way, where conditional probability calculations are kept separate from the logic of the sampler. We discuss strategies for both unit testing and integration testing. As a running example, we show how a Python implementation of Gibbs sampling for a mixture of Gaussians model can be tested.},
	language = {en},
	urldate = {2020-08-21},
	journal = {arXiv:1412.5218 [cs, stat]},
	author = {Grosse, Roger B. and Duvenaud, David K.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.5218},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Machine Learning}
}

@article{leucht_dependent_2013,
	title = {Dependent wild bootstrap for degenerate {U} - and {V} -statistics},
	volume = {117},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X13000304},
	doi = {10.1016/j.jmva.2013.03.003},
	abstract = {Degenerate U - and V -statistics play an important role in the ﬁeld of hypothesis testing since numerous test statistics can be formulated in terms of these quantities. Therefore, consistent bootstrap methods for U - and V -statistics can be applied in order to determine critical values for these tests. We prove a new asymptotic result for degenerate U - and V -statistics of weakly dependent random variables. As our main contribution, we propose a new model-free bootstrap method for U - and V -statistics of dependent random variables. Our method is a modiﬁcation of the dependent wild bootstrap recently proposed by Shao (2010, JASA 105, 218–235), where we do not directly bootstrap the underlying random variables but the summands of the U and V -statistics. Asymptotic theory for the original and the bootstrap statistics is derived under simple and easily veriﬁable conditions. We discuss applications to a Cram´er-von Mises-type test and a two sample test for the marginal distribution of a time series in detail. The ﬁnite sample behavior of the Cram´er-von Mises test is explored in a small simulation study. While the empirical size was reasonably close to the nominal one, we obtained nontrivial empirical power in all cases considered.},
	language = {en},
	urldate = {2020-08-21},
	journal = {Journal of Multivariate Analysis},
	author = {Leucht, Anne and Neumann, Michael H.},
	month = may,
	year = {2013},
	pages = {257--280}
}

@article{kuipers_uniform_2015,
	title = {Uniform random generation of large acyclic digraphs},
	volume = {25},
	issn = {0960-3174, 1573-1375},
	url = {http://arxiv.org/abs/1202.6590},
	doi = {10.1007/s11222-013-9428-y},
	abstract = {Directed acyclic graphs are the basic representation of the structure underlying Bayesian networks, which represent multivariate probability distributions. In many practical applications, such as the reverse engineering of gene regulatory networks, not only the estimation of model parameters but the reconstruction of the structure itself is of great interest. As well as for the assessment of different structure learning algorithms in simulation studies, a uniform sample from the space of directed acyclic graphs is required to evaluate the prevalence of certain structural features. Here we analyse how to sample acyclic digraphs uniformly at random through recursive enumeration, an approach previously thought too computationally involved. Based on complexity considerations, we discuss in particular how the enumeration directly provides an exact method, which avoids the convergence issues of the alternative Markov chain methods and is actually computationally much faster. The limiting behaviour of the distribution of acyclic digraphs then allows us to sample arbitrarily large graphs. Building on the ideas of recursive enumeration based sampling we also introduce a novel hybrid Markov chain with much faster convergence than current alternatives while still being easy to adapt to various restrictions. Finally we discuss how to include such restrictions in the combinatorial enumeration and the new hybrid Markov chain method for efﬁcient uniform sampling of the corresponding graphs.},
	language = {en},
	number = {2},
	urldate = {2020-08-21},
	journal = {Statistics and Computing},
	author = {Kuipers, Jack and Moffa, Giusi},
	month = mar,
	year = {2015},
	note = {arXiv: 1202.6590},
	keywords = {Mathematics - Statistics Theory, Statistics - Computation, Statistics - Machine Learning},
	pages = {227--242}
}

@article{geweke_using_1999,
	title = {Using simulation methods for bayesian econometric models: inference, development,and communication},
	volume = {18},
	issn = {0747-4938, 1532-4168},
	shorttitle = {Using simulation methods for bayesian econometric models},
	url = {http://www.tandfonline.com/doi/abs/10.1080/07474939908800428},
	doi = {10.1080/07474939908800428},
	abstract = {This paper surveys the fundamental principles of subjective Bayesian inference in econometrics and the implementation of those principles using posterior simulation methods. The emphasis is on the combination of models and the development of predictive distributions. Moving beyond conditioning on a fixed number of completely specified models, the paper introduces subjective Bayesian tools for formal comparison of these models with as yet incompletely specified models. The paper then shows how posterior simulators can facilitate communication between investigators (for example, econometricians) on the one hand and remote clients (for example, decision makers) on the other, enabling clients to vary the prior distributions and functions of interest employed by investigators. A theme of the paper is the practicality of subjective Bayesian methods. To this end, the paper describes publicly available software for Bayesian inference, model development, and communication and provides illustrations using two simple econometric models.},
	language = {en},
	number = {1},
	urldate = {2020-08-21},
	journal = {Econometric Reviews},
	author = {Geweke, John},
	month = jan,
	year = {1999},
	pages = {1--73}
}

@article{grzegorczyk_improving_2008,
	title = {Improving the structure {MCMC} sampler for {Bayesian} networks by introducing a new edge reversal move},
	volume = {71},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-008-5057-7},
	doi = {10.1007/s10994-008-5057-7},
	abstract = {Applications of Bayesian networks in systems biology are computationally demanding due to the large number of model parameters. Conventional MCMC schemes based on proposal moves in structure space tend to be too slow in mixing and convergence, and have recently been superseded by proposal moves in the space of node orders. A disadvantage of the latter approach is the intrinsic inability to specify the prior probability on network structures explicitly. The relative paucity of different experimental conditions in contemporary systems biology implies a strong inﬂuence of the prior probability on the posterior probability and, hence, the outcome of inference. Consequently, the paradigm of performing MCMC proposal moves in order rather than structure space is not entirely satisfactory. In the present article, we propose a new and more extensive edge reversal move in the original structure space, and we show that this signiﬁcantly improves the convergence of the classical structure MCMC scheme.},
	language = {en},
	number = {2-3},
	urldate = {2020-08-21},
	journal = {Machine Learning},
	author = {Grzegorczyk, Marco and Husmeier, Dirk},
	month = jun,
	year = {2008},
	pages = {265--305}
}

@article{gretton_kernel_2012,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	url = {http://jmlr.org/papers/v13/gretton12a.html},
	number = {25},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	year = {2012},
	pages = {723--773}
}

@article{sutherland_generative_2019,
	title = {Generative {Models} and {Model} {Criticism} via {Optimized} {Maximum} {Mean} {Discrepancy}},
	url = {http://arxiv.org/abs/1611.04488},
	abstract = {We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: ﬁrst, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model’s samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classiﬁer. This post-publication revision corrects some errors in constants of the estimator (5). The appendix deriving the estimator has been replaced by Sutherland (2019).},
	language = {en},
	urldate = {2020-08-21},
	journal = {arXiv:1611.04488 [cs, stat]},
	author = {Sutherland, Dougal J. and Tung, Hsiao-Yu and Strathmann, Heiko and De, Soumyajit and Ramdas, Aaditya and Smola, Alex and Gretton, Arthur},
	month = jun,
	year = {2019},
	note = {arXiv: 1611.04488},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Statistics - Methodology}
}

@article{madigan_bayesian_1995,
	title = {Bayesian {Graphical} {Models} for {Discrete} {Data}},
	volume = {63},
	issn = {03067734},
	url = {https://www.jstor.org/stable/1403615?origin=crossref},
	doi = {10.2307/1403615},
	abstract = {For more than half a century, data analysts have used graphs to represent statistical models. In particular, graphical "conditional independence" models have emerged as a useful class of models. Applications of such models to probabilistic expert systems, image analysis, and pedigree analysis have motivated much of this work, and several expository texts are now available.},
	language = {en},
	number = {2},
	urldate = {2020-08-21},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Madigan, David and York, Jeremy and Allard, Denis},
	month = aug,
	year = {1995},
	pages = {215}
}

@article{chen_bayesian_2011,
	title = {A {Bayesian} {Lasso} via reversible-jump {MCMC}},
	volume = {91},
	issn = {01651684},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168411000703},
	doi = {10.1016/j.sigpro.2011.02.014},
	abstract = {Variable selection is a topic of great importance in high-dimensional statistical modeling and has a wide range of real-world applications. Many variable selection techniques have been proposed in the context of linear regression, and the Lasso model is probably one of the most popular penalized regression techniques. In this paper, we propose a new, fully hierarchical, Bayesian version of the Lasso model by employing ﬂexible sparsity promoting priors. To obtain the Bayesian Lasso estimate, a reversiblejump MCMC algorithm is developed for joint posterior inference over both discrete and continuous parameter spaces. Simulations demonstrate that the proposed RJ-MCMCbased Bayesian Lasso yields smaller estimation errors and more accurate sparsity pattern detection when compared with state-of-the-art optimization-based Lasso-type methods, a standard Gibbs sampler-based Bayesian Lasso and the Binomial–Gaussian prior model. To demonstrate the applicability and estimation stability of the proposed Bayesian Lasso, we examine a benchmark diabetes data set and real functional Magnetic Resonance Imaging data. As an extension of the proposed RJ-MCMC framework, we also develop an MCMC-based algorithm for the Binomial–Gaussian prior model and illustrate its improved performance over the non-Bayesian estimate via simulations.},
	language = {en},
	number = {8},
	urldate = {2020-08-21},
	journal = {Signal Processing},
	author = {Chen, Xiaohui and Jane Wang, Z. and McKeown, Martin J.},
	month = aug,
	year = {2011},
	pages = {1920--1932}
}

@article{gandy_unit_2020,
	title = {Unit {Testing} for {MCMC} and other {Monte} {Carlo} {Methods}},
	url = {http://arxiv.org/abs/2001.06465},
	abstract = {We propose approaches for testing implementations of Markov Chain Monte Carlo methods as well as of general Monte Carlo methods. Based on statistical hypothesis tests, these approaches can be used in a unit testing framework to, for example, check if individual steps in a Gibbs sampler or a reversible jump MCMC have the desired invariant distribution. Two exact tests for assessing whether a given Markov chain has a speciﬁed invariant distribution are discussed. These and other tests of Monte Carlo methods can be embedded into a sequential method that allows low expected effort if the simulation shows the desired behavior and high power if it does not. Moreover, the false rejection probability can be kept arbitrarily low. For general Monte Carlo methods, this allows testing, for example, if a sampler has a speciﬁed distribution or if a sampler produces samples with the desired mean. The methods have been implemented in the R-package MCUnit.},
	language = {en},
	urldate = {2020-08-21},
	journal = {arXiv:2001.06465 [stat]},
	author = {Gandy, Axel and Scott, James},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.06465},
	keywords = {Statistics - Computation, Statistics - Methodology}
}

@article{geweke_getting_2004,
	title = {Getting {It} {Right}: {Joint} {Distribution} {Tests} of {Posterior} {Simulators}},
	volume = {99},
	issn = {01621459},
	url = {http://www.jstor.org/stable/27590449},
	abstract = {[Analytical or coding errors in posterior simulators can produce reasonable but incorrect approximations of posterior moments. This article develops simple tests of posterior simulators that detect both kinds of errors, and uses them to detect and correct errors in two previously published articles. The tests exploit the fact that a Bayesian model specifies the joint distribution of observables (data) and unobservables (parameters). There are two joint distribution simulators. The marginal-conditional simulator draws unobservables from the prior and then observables conditional on unobservables. The successive-conditional simulator alternates between the posterior simulator and an observables simulator. Formal comparison of moment approximations of the two simulators reveals existing analytical or coding errors in the posterior simulator.]},
	number = {467},
	urldate = {2020-08-21},
	journal = {Journal of the American Statistical Association},
	author = {Geweke, John},
	year = {2004},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {799--804}
}
